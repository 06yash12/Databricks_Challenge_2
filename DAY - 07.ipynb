{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99debe7-94a6-47c7-afbf-a152922ab12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e945b1-d017-4f0e-b55e-3ddd9e92c009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql import functions as F\n",
    "print(\"MLflow setup successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1204ae-80c2-452b-8dbb-6e949b31d81e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "events = spark.table(\"workspace.ecommerce.events_delta\") # Reload bronze events table\n",
    "features_df = spark.table(\"workspace.ecommerce.silver_user_features\") # Reload silver feature table\n",
    "from pyspark.sql import functions as F # Recreate binary label (target)\n",
    "label_df = events.groupBy(\"user_id\").agg(\n",
    "    F.max(\n",
    "        F.when(F.col(\"event_type\") == \"purchase\", 1).otherwise(0)\n",
    "    ).alias(\"purchased\"))\n",
    "training_data = features_df.join(label_df, \"user_id\")  # Join features + label (same as Day 5/6)\n",
    "print(\"Training dataset recreated successfully!\")\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1514ae-ec6c-4501-93be-8fa6a350a50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "train_df, test_df = training_data.randomSplit([0.8, 0.2], seed=42) # Recreate train/test split\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"total_events\", \"total_purchases\", \"total_spent\", \"avg_spent\"],\n",
    "    outputCol=\"features\")\n",
    "train_ml = assembler.transform(train_df) \\\n",
    ".select(\"features\", F.col(\"purchased\").alias(\"label\"))\n",
    "test_ml = assembler.transform(test_df) \\\n",
    ".select(\"features\", F.col(\"purchased\").alias(\"label\"))\n",
    "print(\"Train/Test ML datasets prepared successfully!\")\n",
    "print(\"Train ML count:\", train_ml.count())\n",
    "print(\"Test ML count:\", test_ml.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41255476-4580-4231-b94e-5582635f9129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    seed=42 )\n",
    "rf_model = rf.fit(train_ml)\n",
    "print(\"RandomForest model trained for MLflow logging!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d96fca6f-e246-43d9-b34e-fc8aa07c8753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Generate predictions on test data\n",
    "predictions = rf_model.transform(test_ml)\n",
    "# Create evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "# Calculate AUC\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"AUC calculated for MLflow logging:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11667fa-caa4-42a2-b191-8b47a956f566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_ml.select(\"label\").groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33f0c2fe-9164-4def-94c1-d782baff98d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator  # Recreate predictions safely\n",
    "predictions = rf_model.transform(test_ml)  # Check prediction sample\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show(5, truncate=False)  # Correct evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"Correct AUC for MLflow logging:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "418263fc-63a3-40b7-bada-17a5ac7a3a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MLFLOW_DFS_TMP\"] = \"/Volumes/workspace/ecommerce/ecommerce_data/mlflow_tmp\"\n",
    "print(\"MLflow UC volume temp path configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da137b93-3ce3-4966-b81e-418823b207f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Day7_RandomForest_Model\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"num_trees\", 50)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_param(\"features\", [\"total_events\", \"purchases\", \"total_spent\", \"avg_price\"])\n",
    "    # Log metric (AUC)\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "\n",
    "    # Log Spark ML model\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=rf_model,\n",
    "        artifact_path=\"random_forest_model\"\n",
    "    )\n",
    "print (\"MLflow run logged successfully!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DAY - 07",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
